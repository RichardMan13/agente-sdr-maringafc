import os
import operator
from typing import Annotated, List, TypedDict, Union, Optional
from dotenv import load_dotenv

# LangChain / LangGraph imports
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import SupabaseVectorStore
from supabase.client import create_client
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage, RemoveMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from langchain_community.tools.tavily_search import TavilySearchResults

load_dotenv()

# --- 1. Configura√ß√£o de Clientes ---
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
llm = ChatOpenAI(model="gpt-4o", temperature=0.7)

# --- 2. Defini√ß√£o do Estado ---
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    context: str
    whatsapp_id: str
    intent_is_sale: bool
    nome_torcedor: Optional[str]
    plano_interesse: Optional[str]
    # Estado interno para controle de fluxo
    loop_step: Annotated[int, operator.add] 

# --- 3. Ferramentas (Tools) ---
@tool("retrieve_docs")
def retrieve_docs(query: str):
    """
    Busca documentos relevantes sobre o Maring√° FC, planos de s√≥cio (Maring√° Paix√£o) e jogos.
    Use esta ferramenta para responder perguntas sobre valores, benef√≠cios, datas de jogos e informa√ß√µes institucionais.
    """
    try:
        # Gera embedding da query
        query_embedding = embeddings.embed_query(query)
        
        # Chama RPC no Supabase
        rpc_res = supabase.rpc("match_documents", {
            "query_embedding": query_embedding,
            "match_threshold": 0.5,
            "match_count": 3
        }).execute()
        
        if not rpc_res.data:
            return "Nenhuma informa√ß√£o relevante encontrada no banco de dados."
            
        # Concatena os resultados
        context = "\n\n".join([item['conteudo'] for item in rpc_res.data])
        return context
    except Exception as e:
        return f"Erro ao acessar banco de dados: {str(e)}"

# --- Nova Ferramenta de Busca na Loja ---
@tool("search_store")
def search_store(query: str):
    """
    Busca produtos, pre√ßos e disponibilidade diretamente na loja oficial do Maring√° FC.
    Use esta ferramenta SEMPRE que o usu√°rio perguntar sobre camisas, acess√≥rios ou produtos f√≠sicos.
    URL Base: https://store.maringafc.com/
    """
    search = TavilySearchResults(
        max_results=3,
        search_depth="advanced",
        include_domains=["store.maringafc.com"] # Restringe a busca apenas √† loja oficial
    )
    return search.invoke(query)

# Lista de ferramentas dispon√≠veis para o agente
tools = [retrieve_docs, search_store]
tool_node = ToolNode(tools)

# --- 4. Fun√ß√µes de Apoio (Nodes) ---

def parse_messages(messages):
    """Formata mensagens para string (uso em prompts)."""
    return "\n".join([f"{m.type}: {m.content}" for m in messages])

# --- N√ì: Summarizer ---
def summarize_conversation(state: AgentState):
    """Resume a conversa se ficar muito longa."""
    stored_messages = state['messages']
    
    # Exemplo: mantermos apenas ~6 mensagens recentes sem resumir
    if len(stored_messages) <= 6:
        return {}
    
    # Resume tudo exceto as √∫ltimas 4
    to_summarize = stored_messages[:-4]
    if not to_summarize:
        return {}
        
    summary_message = parse_messages(to_summarize)
    prompt = f"Resuma a conversa entre Torcedor e Dog√£o (SDR Maring√° FC). Mantenha nome e plano de interesse.\n\n{summary_message}"
    
    response = llm.invoke(prompt)
    summary = response.content
    
    delete_messages = [RemoveMessage(id=m.id) for m in to_summarize]
    summary_msg = SystemMessage(content=f"RESUMO ANTERIOR: {summary}")
    
    return {"messages": delete_messages + [summary_msg]}

# --- N√ì: Agent (Router) ---
def agent_node(state: AgentState):
    """
    Analisa a √∫ltima mensagem e decide se chama a ferramenta de busca ou responde direto.
    """
    messages = state['messages']
    
    system_prompt = SystemMessage(content="""Voc√™ √© o "Dog√£o", o SDR (Pr√©-vendas) oficial do Maring√° FC.
    Sua miss√£o √© engajar a torcida e **CONVERTER VENDAS**.
    
    HIERARQUIA DE OBJETIVOS (SDR):
    1. üèÜ VENDER S√ìCIO TORCEDOR: Prioridade m√°xima. Sempre tente conectar o assunto ao plano 'Maring√° Paix√£o'.
    2. üé´/üëï VENDER INGRESSOS E PRODUTOS: Receita imediata.
    3. üìù QUALIFICAR LEAD: Extrair Nome e Interesse para o time de vendas.
    
    DIRETRIZES DE FERRAMENTAS:
    - S√≥cio/Ingressos/Clube: USE 'retrieve_docs'.
    - Camisas/Produtos/Loja: USE 'search_store' para dar pre√ßos e op√ß√µes da loja oficial.
    - Conversa fiada: Responda diretamente.

    COMPORTAMENTO:
    - Seja vibrante, use g√≠rias da torcida (ex: "Pra cima!", "Dog√£o").
    - N√ÉO seja passivo. Tire a d√∫vida e IMEDIATAMENTE fa√ßa uma pergunta de fechamento ou convite (ex: "Bora garantir o S√≥cio hoje?").
    """)
    
    # Filtra system messages antigos para evitar duplica√ß√£o no contexto da LLM, mantendo o resumo se houver
    filtered_msgs = [m for m in messages if not isinstance(m, SystemMessage)]
    # Procura se tem algum resumo (SystemMessage criada pelo summarizer) e mant√©m
    resumos = [m for m in messages if isinstance(m, SystemMessage) and "RESUMO" in str(m.content)]
    
    final_msgs = [system_prompt] + resumos + filtered_msgs
    
    # Bind tools
    model = llm.bind_tools(tools)
    response = model.invoke(final_msgs)
    
    return {"messages": [response]}

# --- N√ì: Grade Documents ---
class GradeResult(BaseModel):
    relevant: bool = Field(description="True se os documentos cont√™m a resposta, False caso contr√°rio")

def grade_documents(state: AgentState):
    """
    Avalia se os documentos trazidos pela ferramenta s√£o suficientes.
    """
    messages = state['messages']
    last_tool_msg = messages[-1]
    
    # Defesa: se n√£o for ToolMessage, segue o fluxo
    if not isinstance(last_tool_msg, ToolMessage):
        return {"context": "", "loop_step": 0}

    docs_content = last_tool_msg.content
    
    # Se a busca n√£o retornou nada √∫til
    if "Nenhuma informa√ß√£o" in docs_content:
        return {"context": docs_content, "loop_step": 1} # Incrementa loop para controle

    # Avalia√ß√£o com LLM
    # Pegamos a √∫ltima pergunta do usu√°rio
    human_msgs = [m for m in messages if isinstance(m, HumanMessage)]
    last_question = human_msgs[-1].content if human_msgs else ""
    
    prompt = f"""Pergunta: {last_question}
    Documentos Recuperados: {docs_content}
    
    Os documentos cont√™m a informa√ß√£o para responder a pergunta? Responda Sim ou N√£o."""
    
    structured = llm.with_structured_output(GradeResult)
    result = structured.invoke(prompt)
    
    # Se relevante, zera o loop. Se n√£o, incrementa.
    step_inc = 0 if result.relevant else 1
    
    #hack: salvamos 'context' explicitamente para o generate_answer usar
    return {"context": docs_content, "loop_step": step_inc}

# --- N√ì: Rewrite Question ---
def rewrite_question(state: AgentState):
    """
    Reescreve a query para tentar melhorar a busca.
    """
    messages = state['messages']
    human_msgs = [m for m in messages if isinstance(m, HumanMessage)]
    original_query = human_msgs[-1].content if human_msgs else ""
    
    prompt = (
        "Analise a pergunta inicial e tente compreender a inten√ß√£o sem√¢ntica subjacente.\n"
        "Aqui est√° a pergunta inicial:"
        "\n ------- \n"
        f"{original_query}"
        "\n ------- \n"
        "Formule uma pergunta de busca otimizada para o banco de dados vetorial (RAG).\n"
        "Retorne APENAS a nova frase de busca, sem explica√ß√µes adicionais."
    )
    
    response = llm.invoke(prompt)
    new_query = response.content
    
    print(f"üîÑ Reescrevendo: '{original_query}' -> '{new_query}'")
    
    # Instru√≠mos o agente a buscar a nova query
    # Usamos uma HumanMessage injetada 'fingindo' que o usu√°rio pediu essa busca espec√≠fica
    msg = HumanMessage(content=f"Por favor, pesquise especificamente por: {new_query}")
    
    return {"messages": [msg]}

# --- N√ì: Generate Answer (RAG) ---
def generate_answer(state: AgentState):
    """
    Gera a resposta final usando o contexto validado.
    """
    context = state['context']
    messages = state['messages']
    
    current_messages = [m for m in messages if not isinstance(m, SystemMessage)]
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© o Dog√£o, mascote e SDR do Maring√° FC.
        Baseado EXCLUSIVAMENTE no contexto abaixo, responda ao torcedor.
        
        CONTEXTO:
        {context}
        
        Se o contexto n√£o tiver a resposta, diga que vai verificar com a diretoria (mas seja simp√°tico).
        """),
        MessagesPlaceholder(variable_name="messages"),
    ])
    
    chain = prompt | llm
    # Passamos as mensagens para manter o fluxo da conversa
    response = chain.invoke({"messages": current_messages, "context": context})
    
    return {"messages": [response]}

# --- N√ì: Lead Tracker (Final) ---
class LeadInfo(BaseModel):
    venda: bool = Field(description="Interesse comercial detectado")
    nome: Optional[str]
    plano: Optional[str]

def classify_and_track(state: AgentState):
    """Extrai inten√ß√µes e salva dados do lead."""
    messages = state['messages']
    if len(messages) < 2:
        return {"intent_is_sale": False}
        
    history = parse_messages(messages[-6:]) # Analisa √∫ltimas mensagens
    
    prompt = f"""Extraia informa√ß√µes do lead da conversa.
    Se o usu√°rio informar nome ou plano, capture.
    
    Hist√≥rico:
    {history}
    """
    
    try:
        structured = llm.with_structured_output(LeadInfo)
        res = structured.invoke(prompt)
        
        updates = {}
        if res.nome and res.nome not in ["Torcedor", "N√£o informado"]:
            updates["nome_torcedor"] = res.nome
        if res.plano and res.plano not in ["A definir", "N√£o informado"]:
            updates["plano_interesse"] = res.plano
            
        # Persist√™ncia
        nome = updates.get("nome_torcedor") or state.get("nome_torcedor") or "Torcedor"
        plano = updates.get("plano_interesse") or state.get("plano_interesse") or "A definir"
        intent = res.venda
        
        if intent or updates:
            print(f"üéØ Atualizando Lead: {nome} | {plano}")
            supabase.table("leads_sdr").upsert({
                "whatsapp_id": state['whatsapp_id'],
                "nome_torcedor": nome,
                "plano_interesse": plano,
                "convertido": False
            }, on_conflict="whatsapp_id").execute()
            
        return {**updates, "intent_is_sale": intent}
        
    except Exception as e:
        print(f"Erro tracking: {e}")
        return {"intent_is_sale": False}

# --- 5. Montagem do Grafo ---
workflow = StateGraph(AgentState)

# Adiciona n√≥s
workflow.add_node("summarizer", summarize_conversation)
workflow.add_node("agent", agent_node)
workflow.add_node("tools", tool_node)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("rewrite", rewrite_question)
workflow.add_node("generate", generate_answer)
workflow.add_node("tracker", classify_and_track)

# Define Entry Point
workflow.set_entry_point("summarizer")

# Arestas
workflow.add_edge("summarizer", "agent")

# Decis√£o do Agent: Tool ou Resposta Direta?
def route_agent(state):
    last_message = state["messages"][-1]
    if last_message.tool_calls:
        return "tools"
    return "tracker"

workflow.add_conditional_edges("agent", route_agent, {
    "tools": "tools",
    "tracker": "tracker"
})

workflow.add_edge("tools", "grade_documents")

# Decis√£o da Grade: Generate ou Rewrite?
def route_grade(state):
    loop_step = state.get("loop_step", 0)
    context = state.get("context", "")
    
    # Se j√° tentamos reescrever (loop > 0) ou se a resposta veio vazia repetidamente, paramos
    # Se loop_step for 1, significa que falhou a primeira e incrementou. Tentamos rewrite.
    # Se loop_step for 2, j√° reescreveu e buscou de novo. Se ainda ruim, desiste e gera com o que tem.
    if loop_step > 1: 
        return "generate"
        
    if "Nenhuma informa√ß√£o" in context:
        return "rewrite"
        
    # Se step inc foi 0 -> generate. Se foi 1 -> rewrite.
    # Mas como 'grade_documents' retorna step incrementado, checamos:
    # Se era 0 e virou 1 -> rewrite.
    # Se era 1 e virou 2 -> generate (abort)
    
    # Vamos simplificar: se loop_step > 0 E context ruim -> rewrite.
    # Se loop_step == 0 e context ruim -> rewrite (loop vira 1).
    if loop_step > 0: # Significa que N√ìS incrementamos agora indicando falha
        return "rewrite"
        
    return "generate"

workflow.add_conditional_edges("grade_documents", route_grade, {
    "rewrite": "rewrite",
    "generate": "generate"
})

workflow.add_edge("rewrite", "agent")
workflow.add_edge("generate", "tracker")
workflow.add_edge("tracker", END)

# Compila√ß√£o
dogao_agent = workflow.compile()